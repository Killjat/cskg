# 工业协议蜜罐系统测试文档

## 1. 测试概述

### 1.1 测试目的

本测试文档旨在指导工业协议蜜罐系统的测试工作，确保系统能够满足需求文档中规定的所有功能和非功能需求。通过全面的测试，验证系统的正确性、稳定性、性能和安全性，为系统的上线和维护提供可靠的依据。

### 1.2 测试范围

测试范围包括工业协议蜜罐系统的所有核心功能和模块：

- 配置管理模块
- 日志管理模块
- 数据包捕获模块
- 设备指纹识别模块
- Web服务模块
- 系统集成测试
- 性能测试
- 安全性测试

### 1.3 测试策略

采用分层测试策略，从单元测试到系统测试，确保系统的各个层次都经过充分测试：

1. **单元测试**：针对每个模块的功能进行测试，验证模块内部逻辑的正确性
2. **集成测试**：测试模块之间的交互，验证模块集成的正确性
3. **系统测试**：测试整个系统的功能，验证系统是否满足需求
4. **性能测试**：测试系统在高负载情况下的性能表现
5. **安全性测试**：测试系统的安全性，验证系统是否存在安全漏洞
6. **回归测试**：在系统变更后，验证原有功能是否仍然正常

## 2. 测试环境

### 2.1 硬件环境

| 硬件类型         | 配置要求               |
|----------------|--------------------|
| CPU            | 至少4核              |
| 内存            | 至少8GB              |
| 存储空间          | 至少100GB            |
| 网络            | 稳定的网络连接，支持端口监听    |
| 网络接口          | 至少一个可用的网络接口        |

### 2.2 软件环境

| 软件类型         | 版本要求               |
|----------------|--------------------|
| 操作系统          | macOS 12+ 或 CentOS 7/8 或 Ubuntu 20.04/22.04 |
| Go语言           | 1.21+              |
| SQLite          | 3.40+              |
| 浏览器           | Chrome 90+ 或 Firefox 88+ |
| 测试工具          | Go Test、curl、Postman、JMeter、Nmap |

### 2.3 测试环境准备

1. **安装依赖**：
   ```bash
   go mod download
   ```

2. **编译项目**：
   ```bash
   go build -o honeypot_server ./cmd/api
   ```

3. **配置文件准备**：
   ```bash
   cp config/config.yaml.example config/config.yaml
   # 根据测试需要修改配置
   ```

4. **创建测试目录**：
   ```bash
   mkdir -p logs data/pcap data/test
   ```

## 3. 单元测试

### 3.1 测试框架

使用Go语言内置的`testing`包进行单元测试，结合`testify`库进行断言：

```bash
# 安装testify库
go get github.com/stretchr/testify/assert
```

### 3.2 配置管理模块测试

**测试文件**：`config/config_test.go`

**测试用例**：

| 测试用例名称         | 测试目的                     | 测试步骤                                | 预期结果                                |
|----------------|--------------------------|-------------------------------------|-------------------------------------|
| 测试配置加载         | 验证配置文件能否正确加载              | 1. 创建测试配置文件<br>2. 调用LoadConfig函数<br>3. 验证返回的配置对象 | 配置对象正确，包含预期的配置值                     |
| 测试默认配置         | 验证默认配置能否正确设置              | 1. 删除配置文件<br>2. 调用LoadConfig函数<br>3. 验证返回的配置对象 | 配置对象包含正确的默认值                       |
| 测试配置项访问        | 验证配置项能否正确访问              | 1. 设置测试配置<br>2. 访问各个配置项<br>3. 验证配置值 | 配置项值与预期一致                           |

**测试代码示例**：

```go
package config

import (
    "os"
    "testing"

    "github.com/stretchr/testify/assert"
)

func TestLoadConfig(t *testing.T) {
    // 创建测试配置文件
    testConfig := `
honeypot:
  name: "Test Honeypot"
  version: "1.0.0"
  log_path: "./test_logs"
  log_level: "debug"
`
    
    // 写入测试配置文件
    os.WriteFile("./test_config.yaml", []byte(testConfig), 0644)
    defer os.Remove("./test_config.yaml")
    
    // 加载配置
    config, err := LoadConfig(".")
    assert.NoError(t, err)
    assert.NotNil(t, config)
    
    // 验证配置值
    assert.Equal(t, "Test Honeypot", config.Honeypot.Name)
    assert.Equal(t, "debug", config.Honeypot.LogLevel)
}
```

### 3.3 日志管理模块测试

**测试文件**：`internal/logger/logger_test.go`

**测试用例**：

| 测试用例名称         | 测试目的                     | 测试步骤                                | 预期结果                                |
|----------------|--------------------------|-------------------------------------|-------------------------------------|
| 测试日志创建         | 验证能否正确创建日志实例            | 1. 调用NewLogger函数<br>2. 验证返回的日志实例 | 日志实例非空，能够正常使用                      |
| 测试不同日志级别       | 验证不同日志级别能否正确输出          | 1. 创建日志实例<br>2. 输出不同级别的日志<br>3. 检查日志文件 | 日志文件中包含相应级别的日志条目                   |
| 测试日志文件滚动       | 验证日志文件能否正确滚动            | 1. 设置较小的日志文件大小<br>2. 输出大量日志<br>3. 检查日志文件数量 | 生成多个日志文件，旧文件被压缩                       |

### 3.4 设备指纹识别模块测试

**测试文件**：`internal/device/fingerprint_test.go`

**测试用例**：

| 测试用例名称         | 测试目的                     | 测试步骤                                | 预期结果                                |
|----------------|--------------------------|-------------------------------------|-------------------------------------|
| 测试设备指纹生成       | 验证能否正确生成设备指纹            | 1. 创建FingerprintManager实例<br>2. 调用CaptureFingerprint函数<br>3. 验证生成的设备指纹 | 设备指纹包含正确的IP、端口、协议等信息               |
| 测试设备识别         | 验证能否正确识别设备信息            | 1. 创建带有设备信息的测试数据<br>2. 调用identifyDevice函数<br>3. 验证识别结果 | 设备信息识别正确，包含预期的制造商、型号等信息             |
| 测试数据库操作        | 验证数据库操作能否正确执行           | 1. 创建FingerprintManager实例<br>2. 保存设备指纹<br>3. 从数据库加载设备指纹<br>4. 验证加载结果 | 设备指纹能够正确保存和加载                       |

### 3.5 Web服务模块测试

**测试文件**：`internal/web/server_test.go`

**测试用例**：

| 测试用例名称         | 测试目的                     | 测试步骤                                | 预期结果                                |
|----------------|--------------------------|-------------------------------------|-------------------------------------|
| 测试API接口响应       | 验证API接口能否正确响应           | 1. 创建Server实例<br>2. 发送HTTP请求<br>3. 验证响应结果 | API接口返回正确的状态码和响应数据                   |
| 测试HTML模板渲染       | 验证HTML模板能否正确渲染           | 1. 创建Server实例<br>2. 请求HTML页面<br>3. 验证HTML内容 | HTML页面渲染正确，包含预期的内容                   |
| 测试统计信息计算       | 验证统计信息能否正确计算           | 1. 创建带有测试数据的FingerprintManager实例<br>2. 调用calculateStats函数<br>3. 验证统计结果 | 统计信息计算正确，包含预期的连接数、设备数等             |

### 3.6 运行单元测试

```bash
# 运行所有单元测试
go test ./...

# 运行特定模块的测试
go test ./internal/device/...

# 运行测试并生成覆盖率报告
go test -cover ./...

# 运行测试并生成详细日志
go test -v ./...
```

## 4. 集成测试

### 4.1 测试目的

验证系统各个模块之间的集成是否正常，确保模块之间的交互能够正确执行。

### 4.2 集成测试用例

#### 4.2.1 数据包捕获与设备指纹识别集成测试

**测试用例名称**：数据包捕获后设备指纹识别

**测试目的**：验证数据包捕获模块能否正确调用设备指纹识别模块，生成设备指纹。

**测试步骤**：
1. 启动系统，启用数据包捕获功能
2. 模拟发送Modbus TCP请求到目标端口
3. 检查系统日志，确认数据包被捕获
4. 检查设备指纹数据库，确认生成了设备指纹
5. 验证设备指纹包含正确的信息

**预期结果**：
- 系统日志中显示数据包被捕获
- 设备指纹数据库中生成了新的设备指纹
- 设备指纹包含正确的IP、端口、协议等信息

#### 4.2.2 设备指纹识别与Web服务集成测试

**测试用例名称**：设备指纹在Web界面展示

**测试目的**：验证设备指纹识别模块生成的设备指纹能否正确显示在Web界面。

**测试步骤**：
1. 启动系统，启用设备指纹识别功能
2. 模拟发送多个协议请求到目标端口
3. 访问Web界面，查看设备指纹列表
4. 验证设备指纹列表包含预期的设备指纹
5. 点击设备指纹详情，验证详情页面包含正确的设备信息

**预期结果**：
- Web界面显示了生成的设备指纹
- 设备指纹列表包含预期的设备指纹
- 设备指纹详情页面包含正确的设备信息

#### 4.2.3 完整流程集成测试

**测试用例名称**：从数据包捕获到Web界面展示的完整流程

**测试目的**：验证从数据包捕获到Web界面展示的完整流程是否正常工作。

**测试步骤**：
1. 启动系统，启用所有功能
2. 模拟发送多种协议请求到目标端口
3. 访问Web界面，查看设备指纹列表和统计信息
4. 访问API接口，获取设备指纹和统计信息
5. 验证Web界面和API返回的数据一致

**预期结果**：
- Web界面显示了所有生成的设备指纹
- 统计信息正确反映了实际的连接情况
- API接口返回的数据与Web界面一致

### 4.3 集成测试执行

```bash
# 启动系统
./honeypot_server

# 模拟发送Modbus请求
python3 -c "from pymodbus.client import ModbusTcpClient; client = ModbusTcpClient('localhost', port=502); client.connect(); client.read_holding_registers(0, 1); client.close()"

# 模拟发送MySQL请求
mysql -h localhost -P 3306 -u test -p -e "SELECT 1;" 2>/dev/null || true

# 模拟发送Redis请求
redis-cli -h localhost -p 6379 ping 2>/dev/null || true

# 访问Web界面
curl -s http://localhost:8080/

# 访问API接口
curl -s http://localhost:8080/api/stats
curl -s http://localhost:8080/api/fingerprints
```

## 5. 系统测试

### 5.1 功能测试

#### 5.1.1 配置管理功能测试

**测试用例名称**：配置文件修改后系统行为变化

**测试目的**：验证修改配置文件后，系统能够正确应用新的配置。

**测试步骤**：
1. 启动系统，使用默认配置
2. 访问Web界面，确认系统运行在默认端口
3. 修改配置文件，更改Web服务端口
4. 重启系统
5. 访问新端口的Web界面，确认系统运行在新端口

**预期结果**：
- 系统能够正确应用新的配置
- Web服务运行在新的端口
- 其他配置项也能够正确应用

#### 5.1.2 数据包捕获功能测试

**测试用例名称**：数据包捕获功能

**测试目的**：验证系统能够正确捕获指定端口的数据包。

**测试步骤**：
1. 启动系统，启用数据包捕获功能
2. 配置数据包捕获保存路径
3. 模拟发送多种协议请求到目标端口
4. 检查数据包保存路径，确认生成了PCAP文件
5. 使用Wireshark打开PCAP文件，验证数据包内容

**预期结果**：
- 数据包保存路径中生成了PCAP文件
- PCAP文件中包含模拟发送的数据包
- 数据包内容与预期一致

#### 5.1.3 设备指纹识别功能测试

**测试用例名称**：设备指纹识别功能

**测试目的**：验证系统能够正确识别设备信息，生成设备指纹。

**测试步骤**：
1. 启动系统，启用设备指纹识别功能
2. 使用不同的设备发送请求到目标端口
3. 访问Web界面，查看设备指纹列表
4. 验证设备指纹包含正确的设备信息
5. 验证不同设备生成了不同的设备指纹

**预期结果**：
- 系统生成了多个设备指纹
- 设备指纹包含正确的设备信息
- 不同设备生成了不同的设备指纹

#### 5.1.4 Web服务功能测试

**测试用例名称**：Web服务功能

**测试目的**：验证Web服务的各项功能是否正常工作。

**测试步骤**：
1. 启动系统，启用Web服务
2. 访问主页，验证页面正常显示
3. 访问设备指纹列表页面，验证列表正常显示
4. 访问设备指纹详情页面，验证详情正常显示
5. 访问连接记录页面，验证记录正常显示
6. 访问统计信息页面，验证统计信息正常显示

**预期结果**：
- 所有页面都能正常访问
- 页面内容与预期一致
- 页面加载速度符合要求

### 5.2 性能测试

#### 5.2.1 并发连接测试

**测试目的**：验证系统在高并发情况下的性能表现。

**测试工具**：JMeter 或 hey

**测试步骤**：
1. 启动系统
2. 使用JMeter创建并发连接测试计划
3. 设置并发用户数（如1000个并发用户）
4. 发送请求到目标端口
5. 监控系统的CPU、内存占用情况
6. 记录系统的响应时间和吞吐量

**预期结果**：
- 系统能够处理1000个并发用户
- 平均响应时间不超过2秒
- CPU和内存占用率在合理范围内
- 没有出现系统崩溃或服务不可用的情况

#### 5.2.2 数据包处理能力测试

**测试目的**：验证系统处理数据包的能力。

**测试工具**：tcpreplay 或 scapy

**测试步骤**：
1. 启动系统，启用数据包捕获功能
2. 使用tcpreplay发送大量数据包到目标端口
3. 监控系统的CPU、内存占用情况
4. 记录系统处理数据包的速度
5. 检查系统日志，确认所有数据包都被处理

**预期结果**：
- 系统能够处理大量数据包
- 数据包处理速度符合要求
- CPU和内存占用率在合理范围内
- 没有出现数据包丢失的情况

#### 5.2.3 Web服务性能测试

**测试目的**：验证Web服务的性能表现。

**测试工具**：JMeter 或 hey

**测试步骤**：
1. 启动系统，启用Web服务
2. 使用JMeter创建Web服务测试计划
3. 设置并发用户数（如1000个并发用户）
4. 发送请求到Web服务
5. 记录Web服务的响应时间和吞吐量
6. 监控系统的CPU、内存占用情况

**预期结果**：
- Web服务能够处理1000个并发用户
- 平均响应时间不超过2秒
- CPU和内存占用率在合理范围内
- 没有出现Web服务崩溃或不可用的情况

### 5.3 安全性测试

#### 5.3.1 蜜罐系统安全性测试

**测试目的**：验证蜜罐系统本身的安全性，防止被用作攻击跳板。

**测试工具**：Nmap、Metasploit

**测试步骤**：
1. 启动系统
2. 使用Nmap扫描系统开放的端口
3. 使用Metasploit尝试对系统进行渗透测试
4. 检查系统日志，确认是否记录了攻击尝试
5. 验证系统没有被成功攻击

**预期结果**：
- 系统只开放了必要的端口
- 系统日志中记录了攻击尝试
- 系统没有被成功攻击
- 系统能够防止被用作攻击跳板

#### 5.3.2 数据安全测试

**测试目的**：验证系统的数据安全性，防止敏感数据泄露。

**测试步骤**：
1. 启动系统，生成一些设备指纹数据
2. 检查数据库文件，确认敏感数据是否加密存储
3. 尝试从数据库中读取敏感数据
4. 验证敏感数据是否加密

**预期结果**：
- 敏感数据在数据库中加密存储
- 无法直接从数据库中读取敏感数据
- 数据访问需要权限验证

#### 5.3.3 网络安全测试

**测试目的**：验证系统的网络安全性，防止网络攻击。

**测试工具**：Nmap、Wireshark

**测试步骤**：
1. 启动系统
2. 使用Nmap扫描系统的网络安全漏洞
3. 使用Wireshark监控系统的网络流量
4. 尝试发送恶意网络数据包到系统
5. 检查系统日志，确认是否记录了恶意数据包

**预期结果**：
- 系统没有明显的网络安全漏洞
- 系统日志中记录了恶意数据包
- 系统能够抵御常见的网络攻击

## 5. 测试执行流程

### 5.1 测试准备

1. 搭建测试环境，安装所需的硬件和软件
2. 准备测试数据和测试工具
3. 编写测试用例，明确测试目的、测试步骤和预期结果
4. 配置系统，确保系统处于测试状态

### 5.2 测试执行

1. 运行单元测试，验证各个模块的功能
2. 运行集成测试，验证模块之间的集成
3. 运行系统测试，验证系统的整体功能
4. 运行性能测试，验证系统的性能表现
5. 运行安全性测试，验证系统的安全性

### 5.3 测试结果分析

1. 收集测试结果，包括测试日志、性能数据、安全扫描结果等
2. 分析测试结果，确认是否符合预期
3. 记录测试中发现的问题和缺陷
4. 对问题和缺陷进行分类和优先级排序
5. 跟踪问题和缺陷的修复情况

### 5.4 测试报告生成

1. 整理测试结果，生成测试报告
2. 测试报告包括以下内容：
   - 测试概述
   - 测试环境
   - 测试用例
   - 测试结果
   - 问题和缺陷
   - 测试结论
   - 建议和改进
3. 提交测试报告给相关人员

## 6. 测试结果分析

### 6.1 测试结果评估标准

| 测试结果         | 评估标准                     |
|----------------|--------------------------|
| 功能测试         | 所有功能测试用例通过率达到100%          |
| 性能测试         | 响应时间不超过2秒，吞吐量符合要求         |
| 安全性测试        | 没有发现高风险漏洞，中低风险漏洞数量在可接受范围内 |
| 稳定性测试        | 系统能够7x24小时稳定运行             |

### 6.2 问题和缺陷分类

| 缺陷等级         | 描述                     | 处理方式                     |
|----------------|--------------------------|--------------------------|
| 致命缺陷          | 导致系统崩溃或无法使用的缺陷          | 立即修复，重新测试                |
| 严重缺陷          | 影响系统主要功能的缺陷              | 优先级高，尽快修复                |
| 一般缺陷          | 影响系统次要功能的缺陷              | 优先级中，安排修复                |
| 轻微缺陷          | 不影响系统功能，仅影响用户体验的缺陷       | 优先级低，可在后续版本修复             |

### 6.3 测试结论

根据测试结果，评估系统是否符合需求文档中的要求：

- 系统功能是否完整实现
- 系统性能是否符合要求
- 系统安全性是否符合要求
- 系统稳定性是否符合要求

## 7. 回归测试

### 7.1 回归测试目的

验证系统在变更后，原有功能是否仍然正常工作，防止引入新的问题。

### 7.2 回归测试触发条件

- 系统代码发生变更
- 系统配置发生变更
- 系统依赖发生变更
- 测试环境发生变更

### 7.3 回归测试用例

从现有测试用例中选择关键的测试用例，组成回归测试用例集：

1. 核心功能测试用例
2. 高频使用功能测试用例
3. 之前发现问题的测试用例
4. 边界条件测试用例

### 7.4 回归测试执行

```bash
# 运行回归测试用例
go test -run TestRegression ./...

# 运行集成回归测试
./run_integration_tests.sh

# 运行系统回归测试
./run_system_tests.sh
```

## 8. 测试报告模板

### 8.1 测试报告基本信息

| 字段名称         | 描述                     |
|----------------|--------------------------|
| 报告编号          | 测试报告的唯一标识符              |
| 测试项目          | 测试的项目名称                 |
| 测试版本          | 测试的系统版本                 |
| 测试时间          | 测试的开始和结束时间              |
| 测试人员          | 参与测试的人员                 |
| 测试环境          | 测试使用的硬件和软件环境             |

### 8.2 测试摘要

| 字段名称         | 描述                     |
|----------------|--------------------------|
| 测试范围          | 测试覆盖的功能和模块              |
| 测试用例数量        | 执行的测试用例数量               |
| 通过测试用例数量      | 通过的测试用例数量               |
| 失败测试用例数量      | 失败的测试用例数量               |
| 通过率           | 通过测试用例数量 / 总测试用例数量      |
| 主要问题          | 测试中发现的主要问题              |
| 测试结论          | 测试的总体结论                 |

### 8.3 测试详细结果

| 测试模块         | 测试用例数量 | 通过数量 | 失败数量 | 通过率 | 主要问题                     |
|----------------|---------|------|------|-----|--------------------------|
| 配置管理模块        | 10      | 10   | 0    | 100% | 无                        |
| 日志管理模块        | 15      | 14   | 1    | 93%  | 日志文件滚动功能存在问题            |
| 数据包捕获模块       | 20      | 18   | 2    | 90%  | 某些协议数据包解析错误             |
| 设备指纹识别模块      | 25      | 23   | 2    | 92%  | 部分设备识别不准确               |
| Web服务模块        | 30      | 28   | 2    | 93%  | 某些页面加载速度慢               |
| 系统集成          | 15      | 14   | 1    | 93%  | 模块间通信存在延迟               |
| 性能测试          | 10      | 9    | 1    | 90%  | 高并发下响应时间过长              |
| 安全性测试         | 15      | 13   | 2    | 87%  | 存在一些低风险安全漏洞             |

### 8.4 问题和缺陷列表

| 缺陷ID         | 缺陷描述                     | 缺陷等级 | 所属模块         | 修复状态 | 修复建议                     |
|---------------|--------------------------|--------|----------------|--------|--------------------------|
| DEF-001        | 日志文件滚动功能存在问题             | 一般    | 日志管理模块        | 待修复    | 检查日志文件滚动逻辑，修复bug       |
| DEF-002        | 某些协议数据包解析错误             | 严重    | 数据包捕获模块       | 待修复    | 完善协议解析逻辑，增加错误处理         |
| DEF-003        | 部分设备识别不准确               | 一般    | 设备指纹识别模块      | 待修复    | 优化设备识别算法，提高识别准确率       |
| DEF-004        | 某些页面加载速度慢               | 一般    | Web服务模块        | 待修复    | 优化页面渲染，增加缓存机制           |
| DEF-005        | 模块间通信存在延迟               | 一般    | 系统集成          | 待修复    | 优化模块间通信机制，减少延迟          |
| DEF-006        | 高并发下响应时间过长              | 严重    | 性能测试          | 待修复    | 优化系统性能，增加并发处理能力         |
| DEF-007        | 存在一些低风险安全漏洞             | 轻微    | 安全性测试         | 待修复    | 修复低风险安全漏洞，提高系统安全性       |

### 8.5 测试结论和建议

**测试结论**：
- 系统基本功能已经实现，能够满足需求文档中的要求
- 系统性能基本符合要求，但在高并发情况下需要优化
- 系统安全性基本符合要求，但存在一些低风险漏洞
- 系统稳定性良好，能够长时间稳定运行

**建议和改进**：
1. 修复测试中发现的问题和缺陷
2. 优化系统性能，提高高并发处理能力
3. 加强系统安全性，修复低风险安全漏洞
4. 完善系统文档，提高系统的可维护性
5. 增加自动化测试，提高测试效率
6. 定期进行安全审计，确保系统安全性

## 9. 总结

本测试文档详细介绍了工业协议蜜罐系统的测试工作，包括测试策略、测试环境、测试用例、测试执行流程、测试结果分析等内容。通过全面的测试，可以确保系统能够满足需求文档中的所有要求，保证系统的正确性、稳定性、性能和安全性。

测试工作是系统开发过程中的重要环节，能够帮助发现系统中的问题和缺陷，提高系统的质量和可靠性。在测试过程中，应严格按照测试文档的要求进行测试，确保测试结果的准确性和可靠性。同时，应及时记录测试结果，生成测试报告，为系统的上线和维护提供可靠的依据。