import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
from typing import Optional, List

# å¯é€‰ï¼šå¤„ç†åŠ¨æ€åŠ è½½çš„å¤‡æ¡ˆä¿¡æ¯ï¼ˆéœ€å®‰è£… seleniumï¼‰
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False

def clean_text(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬ï¼šå»é™¤å¤šä½™ç©ºæ ¼ã€åˆ¶è¡¨ç¬¦ã€æ¢è¡Œï¼Œç»Ÿä¸€å…¨è§’/åŠè§’"""
    if not text:
        return ""
    # å»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
    text = re.sub(r"\s+", "", text)
    # å…¨è§’è½¬åŠè§’
    text = text.translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789'))
    return text

def get_page_text(target_url: str, use_selenium: bool = False) -> Optional[str]:
    """
    è·å–ç½‘é¡µæ–‡æœ¬ï¼ˆä¼˜å…ˆé™æ€çˆ¬å–ï¼Œå¤±è´¥/æŒ‡å®šæ—¶ç”¨Seleniumå¤„ç†åŠ¨æ€åŠ è½½ï¼‰
    :param target_url: ç›®æ ‡URL
    :param use_selenium: æ˜¯å¦ä½¿ç”¨Selenium
    :return: æ¸…ç†åçš„ç½‘é¡µæ–‡æœ¬
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Accept-Encoding": "gzip, deflate",
        "Connection": "keep-alive"
    }

    # 1. é™æ€çˆ¬å–
    if not use_selenium:
        try:
            response = requests.get(
                target_url,
                headers=headers,
                timeout=10,
                allow_redirects=True
            )
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            soup = BeautifulSoup(response.text, "html.parser")
            
            # ä¼˜å…ˆæå–å¤‡æ¡ˆç›¸å…³æ ‡ç­¾ï¼ˆå¸¸è§çš„å¤‡æ¡ˆå®¹å™¨ï¼‰
            beian_tags = soup.find_all(
                ["footer", "div", "p"],
                attrs={
                    "class": re.compile(r"beian|å¤‡æ¡ˆ|footer", re.IGNORECASE),
                    "id": re.compile(r"beian|å¤‡æ¡ˆ|footer", re.IGNORECASE)
                }
            )
            # åˆå¹¶å¤‡æ¡ˆæ ‡ç­¾æ–‡æœ¬ + å…¨é¡µæ–‡æœ¬ï¼ˆåŒé‡ä¿éšœï¼‰
            beian_text = "".join([tag.get_text() for tag in beian_tags])
            full_text = soup.get_text()
            total_text = beian_text + full_text
            return clean_text(total_text)
        except Exception as e:
            print(f"é™æ€çˆ¬å–å¤±è´¥ï¼š{e}")
            if SELENIUM_AVAILABLE:
                print("å°è¯•ä½¿ç”¨SeleniumåŠ¨æ€çˆ¬å–...")
            else:
                return None

    # 2. åŠ¨æ€çˆ¬å–ï¼ˆSeleniumï¼‰
    if SELENIUM_AVAILABLE:
        try:
            chrome_options = Options()
            chrome_options.add_argument("--headless")  # æ— å¤´æ¨¡å¼ï¼ˆä¸æ˜¾ç¤ºæµè§ˆå™¨ï¼‰
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            driver = webdriver.Chrome(options=chrome_options)
            driver.set_page_load_timeout(15)
            driver.get(target_url)
            # ä¼˜å…ˆæå–å¤‡æ¡ˆæ ‡ç­¾æ–‡æœ¬
            beian_elements = driver.find_elements(
                "xpath",
                "//*[contains(@class, 'beian') or contains(@id, 'beian') or contains(text(), 'å¤‡æ¡ˆ') or self::footer]"
            )
            beian_text = "".join([elem.text for elem in beian_elements])
            full_text = driver.page_source
            soup = BeautifulSoup(full_text, "html.parser")
            total_text = beian_text + soup.get_text()
            driver.quit()
            return clean_text(total_text)
        except Exception as e:
            print(f"Seleniumçˆ¬å–å¤±è´¥ï¼š{e}")
            return None
    return None

def extract_icp_beian(text: str) -> dict:
    """
    ç²¾å‡†æå–ICPå¤‡æ¡ˆå·ã€å…¬å®‰å¤‡æ¡ˆå·
    :param text: æ¸…ç†åçš„ç½‘é¡µæ–‡æœ¬
    :return: å¤‡æ¡ˆä¿¡æ¯å­—å…¸
    """
    result = {
        "icp_record": [],       # ICPå¤‡æ¡ˆå·ï¼ˆå»é‡ï¼‰
        "police_record": [],    # å…¬å®‰å¤‡æ¡ˆå·ï¼ˆå»é‡ï¼‰
        "record_owner": None    # å¤‡æ¡ˆä¸»ä½“
    }

    # 1. ä¼˜åŒ–åçš„ICPå¤‡æ¡ˆå·æ­£åˆ™ï¼ˆå…¼å®¹ç©ºæ ¼ã€åˆ†éš”ç¬¦ã€æ—§å·æ®µï¼‰
    # åŒ¹é…è§„åˆ™ï¼š[çœå¸‚ç®€ç§°] + ä»»æ„å­—ç¬¦ + ICPå¤‡ + æ•°å­— + å· + å¯é€‰åç¼€
    icp_pattern = re.compile(
        r"(äº¬|æ²ª|ç²¤|è‹|æµ™|é²|å·|æ¸|æ´¥|å†€|æ™‹|è’™|è¾½|å‰|é»‘|çš–|é—½|èµ£|è±«|é„‚|æ¹˜|æ¡‚|ç¼|è´µ|äº‘|é™•|ç”˜|é’|å®|æ–°|æ¸¯|æ¾³|å°)"
        r".*?ICPå¤‡.*?(\d{6,8})(?:å·)?(?:-(\d+))?",
        re.IGNORECASE
    )
    icp_matches = icp_pattern.findall(text)
    # æ ¼å¼åŒ–ICPå¤‡æ¡ˆå·ï¼ˆç»Ÿä¸€æ ¼å¼ï¼šçœå¸‚+ICPå¤‡+æ•°å­—+å·+åç¼€ï¼‰
    for match in icp_matches:
        province = match[0]
        num = match[1]
        suffix = match[2] if match[2] else ""
        icp_no = f"{province}ICPå¤‡{num}å·"
        if suffix:
            icp_no += f"-{suffix}"
        result["icp_record"].append(icp_no.upper())

    # 2. ä¼˜åŒ–åçš„å…¬å®‰å¤‡æ¡ˆå·æ­£åˆ™ï¼ˆå…¼å®¹ç©ºæ ¼ã€ä¸åŒä½æ•°ï¼‰
    police_pattern = re.compile(
        r"(äº¬|æ²ª|ç²¤|è‹|æµ™|é²|å·|æ¸|æ´¥|å†€|æ™‹|è’™|è¾½|å‰|é»‘|çš–|é—½|èµ£|è±«|é„‚|æ¹˜|æ¡‚|ç¼|è´µ|äº‘|é™•|ç”˜|é’|å®|æ–°|æ¸¯|æ¾³|å°)"
        r".*?å…¬ç½‘å®‰å¤‡.*?(\d{6,12})(?:å·)?",
        re.IGNORECASE
    )
    police_matches = police_pattern.findall(text)
    # æ ¼å¼åŒ–å…¬å®‰å¤‡æ¡ˆå·
    for match in police_matches:
        province = match[0]
        num = match[1]
        police_no = f"{province}å…¬ç½‘å®‰å¤‡{num}å·"
        result["police_record"].append(police_no.upper())

    # 3. ä¼˜åŒ–å¤‡æ¡ˆä¸»ä½“æå–ï¼ˆå…¼å®¹æ›´å¤šæ ¼å¼ï¼‰
    owner_pattern = re.compile(
        r"(?:ä¸»åŠå•ä½|ç½‘ç«™ä¸»åŠè€…|ç‰ˆæƒæ‰€æœ‰|Â©).*?:?([^ï¼Œã€‚ï¼›ï¼ï¼Ÿ]{2,50})",
        re.IGNORECASE
    )
    owner_matches = owner_pattern.findall(text)
    if owner_matches:
        # è¿‡æ»¤æ— æ•ˆä¸»ä½“ï¼Œä¿ç•™ä¼ä¸š/ä¸ªäººåç§°
        valid_owners = [
            owner for owner in owner_matches
            if not re.match(r"^\d+$", owner) and len(owner) > 2
        ]
        if valid_owners:
            result["record_owner"] = valid_owners[0].strip()

    # å»é‡
    result["icp_record"] = list(set(result["icp_record"]))
    result["police_record"] = list(set(result["police_record"]))
    return result

def extract_webpage_icp_info(target_url: str, use_selenium: bool = False) -> dict:
    """
    æå–ç›®æ ‡ç½‘é¡µå¤‡æ¡ˆä¿¡æ¯ï¼ˆä¸»å‡½æ•°ï¼‰
    :param target_url: ç›®æ ‡URL
    :param use_selenium: æ˜¯å¦ä½¿ç”¨Seleniumå¤„ç†åŠ¨æ€åŠ è½½
    :return: æœ€ç»ˆç»“æœ
    """
    final_result = {
        "icp_record": [],
        "police_record": [],
        "record_owner": None,
        "error": None
    }

    # 1. è·å–ç½‘é¡µæ–‡æœ¬
    page_text = get_page_text(target_url, use_selenium)
    if not page_text:
        final_result["error"] = "ç½‘é¡µæ–‡æœ¬æå–å¤±è´¥"
        return final_result

    # 2. æå–å¤‡æ¡ˆä¿¡æ¯
    beian_info = extract_icp_beian(page_text)
    final_result.update(beian_info)
    return final_result

# ===================== ç¤ºä¾‹è°ƒç”¨ =====================
if __name__ == "__main__":
    # æµ‹è¯•URLï¼ˆæ›¿æ¢ä¸ºä½ çš„ç›®æ ‡URLï¼‰
    target_url = "https://www.tipray.com/product_cont2.php?id=196https://www.tipray.com/&sdclkid=ALf615fsbrDNbJDzb_&bd_vid=9484997432240655851"
    
    # ç¬¬ä¸€æ­¥ï¼šé™æ€æå–ï¼ˆä¼˜å…ˆï¼‰
    icp_info = extract_webpage_icp_info(target_url)
    
    # ç¬¬äºŒæ­¥ï¼šè‹¥é™æ€æå–ä¸åˆ°ï¼Œå°è¯•åŠ¨æ€æå–ï¼ˆéœ€å®‰è£…Seleniumï¼‰
    if not icp_info["icp_record"] and not icp_info["police_record"] and SELENIUM_AVAILABLE:
        icp_info = extract_webpage_icp_info(target_url, use_selenium=True)

    # æ ¼å¼åŒ–è¾“å‡º
    print("=" * 60)
    print(f"ç›®æ ‡URLï¼š{target_url}")
    print("=" * 60)
    if icp_info["error"]:
        print(f"âŒ é”™è¯¯ï¼š{icp_info['error']}")
    else:
        print(f"ğŸŒ ICPå¤‡æ¡ˆå·ï¼š{', '.join(icp_info['icp_record']) or 'æ— '}")
        print(f"ğŸš¨ å…¬å®‰å¤‡æ¡ˆå·ï¼š{', '.join(icp_info['police_record']) or 'æ— '}")
        print(f"ğŸ¢ å¤‡æ¡ˆä¸»ä½“ï¼š{icp_info['record_owner'] or 'æ— '}")
    print("=" * 60)